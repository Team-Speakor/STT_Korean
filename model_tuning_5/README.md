dataset : 38000ê°œ ( 24000/ 3000/ 3000)

### ğŸ› ï¸ TrainingArguments ì„¤ì •í‘œ

| í•­ëª©             | ë³€ê²½ ì „ (v6) | ë³€ê²½ í›„ (v7) | ì„¤ëª…                                         |
| ---------------- | ------------ | ------------ | -------------------------------------------- |
| ë°ì´í„°ì…‹ í¬ê¸°    | 30,000ê°œ     | **38,000ê°œ** | í•™ìŠµì— ë” ë§ì€ ë‹¤ì–‘ì„±ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ í™•ë³´     |
| num_proc (GPUë‹¹) | 4            | 1            | 4 ìœ ì§€í•˜ê³  ë°ì´í„°ì…‹ ëŠ˜ë¦¼ -> í„°ì§ -> 1ë¡œ ì¤„ì„ |

### code

```python
training_args = TrainingArguments(
    output_dir="./wav2vec2_finetune_ko_v6",
    eval_steps=500,
    save_steps=500,
    logging_steps=50,
    eval_strategy="steps",
    save_total_limit=3,
    num_train_epochs=20,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,
    fp16=True,                             # AMP ì‚¬ìš©
    group_by_length=True,                  # ì˜¤ë””ì˜¤ ê¸¸ì´ ê¸°ë°˜ ë™ì  ë°°ì¹˜ êµ¬ì„±
    weight_decay=0.005,
    learning_rate=2e-4,                    # ì‚´ì§ ë‚®ì¶¤
    warmup_ratio=0.05,
    dataloader_num_workers=4,
    report_to="none",                      # WandB ì•ˆ ì“¸ ë•Œ
)
```

## PLAN
