#!/usr/bin/env python
# coding: utf-8
"""
ëª©í‘œ
1) ë¬¸ì¥ë¶€í˜¸ê°€ [UNK] ë¡œ ì¶œë ¥ë˜ëŠ” ë¬¸ì œ í•´ê²° â†’ í† í¬ë‚˜ì´ì €ì— ë¬¸ì¥ë¶€í˜¸(.,?!) ì¶”ê°€
2) ì´ ìƒ˜í”Œ 8000ê°œë§Œ ì‚¬ìš©, ë¹„ìœ¨ 8:1:1
3) í•˜ì´í¼íŒŒë¼ë¯¸í„° & í•™ìŠµ ì˜µì…˜ íŠœë‹ (í•œêµ­ì–´-ì²­ì ì…ì¥ì— ë§ì¶¤)
4) ì ì¬ì  ì˜¤ë¥˜ ì˜ˆë°© (seed, fp16, resume, logging ë“±)
"""

import os, json, random
from dataclasses import dataclass

import torch
from datasets import Dataset, DatasetDict, Audio
from transformers import (
    Wav2Vec2Processor, Wav2Vec2ForCTC,
    TrainingArguments, Trainer, set_seed
)

import numpy as np
import evaluate

# evaluate function
# Hugging Face metric ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
wer_metric = evaluate.load("wer")
cer_metric = evaluate.load("cer")

def compute_metrics(pred):
    pred_logits = pred.predictions
    pred_ids = np.argmax(pred_logits, axis=-1)

    # label_ids = -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ëœ ë¶€ë¶„ì€ tokenizer pad_tokenìœ¼ë¡œ êµì²´
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    # í† í° ID â†’ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

    wer = wer_metric.compute(predictions=pred_str, references=label_str)
    cer = cer_metric.compute(predictions=pred_str, references=label_str)

    return {
        "wer": wer,
        "cer": cer,
    }


# ====== 0. ì¬í˜„ ê°€ëŠ¥ì„± ======  # â˜… ìˆ˜ì •
set_seed(42)

# ====== 1. ê²½ë¡œ ======
BASE_DIR  = "/home/coop1964/STT_Korean/dataset"
LABEL_DIR = os.path.join(BASE_DIR, "script")
VOICE_DIR = os.path.join(BASE_DIR, "audio")

# ====== 2. JSON + WAV ë¡œë“œ ======
data_list = []
for jf in os.listdir(LABEL_DIR):
    if not jf.endswith(".json"): 
        continue
    with open(os.path.join(LABEL_DIR, jf), encoding="utf-8") as f:
        meta = json.load(f)

    wav = os.path.join(VOICE_DIR, meta["fileName"])
    txt = meta.get("transcription", "").strip()

    if os.path.exists(wav) and txt:
        data_list.append({"audio": wav, "text": txt})

# 2-â‘ : ì´ 8 000ê°œ ìƒ˜í”Œë§Œ ëœë¤ ì„ íƒ  # â˜… ìˆ˜ì •
random.shuffle(data_list)
data_list = data_list[:8_000]

full_ds = Dataset.from_list(data_list)

# ====== 3. split 8:1:1 ======
split1 = full_ds.train_test_split(test_size=0.2, seed=42)
split2 = split1["test"].train_test_split(test_size=0.5, seed=42)
dataset = DatasetDict({
    "train":      split1["train"],
    "validation": split2["train"],
    "test":       split2["test"]
})

# ====== 4. ì˜¤ë””ì˜¤ ì»¬ëŸ¼ â†’ 16 kHz ======
dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))

# ====== 5. Processor & í† í¬ë‚˜ì´ì € í™•ì¥ ======
MODEL_NAME = "kresnik/wav2vec2-large-xlsr-korean"
processor  = Wav2Vec2Processor.from_pretrained(MODEL_NAME)

# 5-â‘ : ë¬¸ì¥ë¶€í˜¸ í† í° ì¶”ê°€ â†’ UNK ë¬¸ì œ í•´ê²°   # â˜… ìˆ˜ì •
new_tokens = list(".,?!")
num_added  = processor.tokenizer.add_tokens(new_tokens)
if num_added:
    print(f"ğŸ†• tokenizerì— ë¬¸ì¥ë¶€í˜¸ {new_tokens} ì¶”ê°€ë¨({num_added})")

# ====== 6. ì „ì²˜ë¦¬ ======
def prepare_batch(batch):
    audio = batch["audio"]
    batch["input_values"] = processor(
        audio["array"], sampling_rate=16_000
    ).input_values[0]

    with processor.as_target_processor():
        batch["labels"] = processor(
            batch["text"], return_tensors="pt"
        ).input_ids.squeeze(0)
    return batch

dataset = dataset.map(
    prepare_batch,
    remove_columns=["audio", "text"],
    num_proc=4
)

# ====== 7. DataCollator ======
@dataclass
class DataCollatorCTCWithPadding:
    processor: Wav2Vec2Processor
    padding: bool=True

    def __call__(self, features):
        inputs  = [torch.tensor(f["input_values"]) for f in features]
        labels  = [torch.tensor(f["labels"])       for f in features]

        batch_inputs = torch.nn.utils.rnn.pad_sequence(
            inputs, batch_first=True
        )
        batch_labels = torch.nn.utils.rnn.pad_sequence(
            labels, batch_first=True,
            padding_value=-100
        )
        return {
            "input_values": batch_inputs,
            "labels":       batch_labels
        }

data_collator = DataCollatorCTCWithPadding(processor)

# ====== 8. ëª¨ë¸ ë¡œë“œ ======
model = Wav2Vec2ForCTC.from_pretrained(
    MODEL_NAME,
    vocab_size=len(processor.tokenizer),
    pad_token_id=processor.tokenizer.pad_token_id,
    attention_dropout=0.05,      # â˜… ì•½ê°„ ì™„í™”
    hidden_dropout=0.05,
    feat_proj_dropout=0.0,
    layerdrop=0.05,
    mask_time_prob=0.04,
    ignore_mismatched_sizes=True
)

# í† í¬ë‚˜ì´ì €ê°€ í™•ì¥ëë‹¤ë©´ lm_head ë¦¬ì‚¬ì´ì¦ˆ
if num_added:
    # model.resize_token_embeddings(len(processor.tokenizer))
    print("resize_token")

# ====== 9. í•˜ì´í¼íŒŒë¼ë¯¸í„° ======   # â˜… ìˆ˜ì •
training_args = TrainingArguments(
    output_dir="./wav2vec2_finetune_ko_v3",
    evaluation_strategy="no",
    eval_steps=200,
    save_steps=200,
    logging_steps=50,
    save_total_limit=3,
    num_train_epochs=10,                   # ë°ì´í„° 8 k â†’ 10epochë©´ ì¶©ë¶„
    per_device_train_batch_size=8,         # 4Ã—3090 ì´ë¯€ë¡œ ì—¬ìœ  â†‘
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,         # ì‹¤íš¨ BS=16
    fp16=True,                             # AMP ì‚¬ìš©
    learning_rate=1e-4,                    # ì‚´ì§ ë‚®ì¶¤
    warmup_ratio=0.05,
    dataloader_num_workers=4,
    report_to="none",                      # WandB ì•ˆ ì“¸ ë•Œ
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=processor.feature_extractor,
    compute_metrics=compute_metrics  # âœ… ì¶”ê°€
)

# ====== 10. í•™ìŠµ ======
trainer.train()

# ====== 11. ì €ì¥ ======
CKPT_DIR = "./wav2vec2_korean_v3"
model.save_pretrained(CKPT_DIR)
processor.save_pretrained(CKPT_DIR)
print(f"âœ… ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œê°€ {CKPT_DIR} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# 12. evaluate
eval_results = trainer.evaluate(dataset["validation"])

print("\n========== ëª¨ë¸ í‰ê°€ ê²°ê³¼ ==========")
print(f"í‰ê°€ ì†ì‹¤ (eval_loss): {eval_results.get('eval_loss'):.4f}")
print(f"í‰ê°€ WER (eval_wer): {eval_results.get('wer'):.4f}")
print(f"í‰ê°€ CER (eval_cer): {eval_results.get('cer'):.4f}")
print(f"í‰ê°€ ëŸ°íƒ€ì„ (ì´ˆ): {eval_results.get('eval_runtime'):.2f}")
print(f"ì´ˆë‹¹ í‰ê°€ ìƒ˜í”Œ ìˆ˜: {eval_results.get('eval_samples_per_second'):.2f}")
print(f"ì´ˆë‹¹ í‰ê°€ ìŠ¤í… ìˆ˜: {eval_results.get('eval_steps_per_second'):.2f}")
print(f"í‰ê°€ ì—í¬í¬: {eval_results.get('epoch'):.2f}")
print("====================================")
