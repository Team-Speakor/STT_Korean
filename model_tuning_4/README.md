dataset : 30000ê°œ ( 24000/ 3000/ 3000)

### ðŸ› ï¸ TrainingArguments ì„¤ì •í‘œ

| í•­ëª©                    | ë³€ê²½ ì „ (v5) | ë³€ê²½ í›„ (v6)  | ì„¤ëª…                                                 |
| ----------------------- | ------------ | ------------- | ---------------------------------------------------- |
| ë°ì´í„°ì…‹ í¬ê¸°           | 8,000ê°œ      | **30,000ê°œ**  | í•™ìŠµì— ë” ë§Žì€ ë‹¤ì–‘ì„±ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ í™•ë³´             |
| `learning_rate`         | `1e-4`       | **`2e-4`**    | í•™ìŠµ ìˆ˜ë ´ ì†ë„ í–¥ìƒ ëª©ì  (cosine scheduler í¬í•¨)     |
| `num_train_epochs`      | 20           | 20            | ì¶©ë¶„í•œ epoch ìœ ì§€ë¡œ ê³¼ì†Œ í•™ìŠµ ë°©ì§€                   |
| `eval_strategy`         | `"no"`       | **`"steps"`** | ì •ê¸°ì  ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥ (OOMì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¡œ ì œì–´) |
| `gradient_accumulation` | 8            | 16            | ì‹¤íš¨ batch size ìœ ì§€ (2Ã—16=32)                       |
| `batch_size` (GPUë‹¹)    | 4            | 2             | ë©”ëª¨ë¦¬ ì•ˆì „í•˜ê²Œ ìœ ì§€                                 |

### code

```python
training_args = TrainingArguments(
    output_dir="./wav2vec2_finetune_ko_v6",
    eval_steps=500,
    save_steps=500,
    logging_steps=50,
    eval_strategy="steps",
    save_total_limit=3,
    num_train_epochs=20,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=16,
    fp16=True,                             # AMP ì‚¬ìš©
    group_by_length=True,                  # ì˜¤ë””ì˜¤ ê¸¸ì´ ê¸°ë°˜ ë™ì  ë°°ì¹˜ êµ¬ì„±
    weight_decay=0.005,
    learning_rate=2e-4,                    # ì‚´ì§ ë‚®ì¶¤
    warmup_ratio=0.05,
    dataloader_num_workers=4,
    report_to="none",                      # WandB ì•ˆ ì“¸ ë•Œ
)
```

## PLAN

1. dataset 30000ê°œ -> 38000ê°œ
