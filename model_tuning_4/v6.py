"""
ëª©í‘œ
1)  ë¬¸ì¥ë¶€í˜¸ [UNK] ë¬¸ì œ â†’ í† í¬ë‚˜ì´ì €ì— . , ? ! ì¶”ê°€
2)  ì´ 30 000ê°œ ìƒ˜í”Œ, 8 : 1 : 1 split
3)  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • (epoch 18, lr 2e-4, batch size 32-equiv)
4)  ì¬í˜„ì„±Â·ì•ˆì •ì„±(seeds, fp16, checkpoint, tokenizer resize)
"""

# ============ ë¼ì´ë¸ŒëŸ¬ë¦¬ ============
import os, json, random
from dataclasses import dataclass

import torch, numpy as np
from datasets import Dataset, DatasetDict, Audio
from transformers import (
    Wav2Vec2Processor, Wav2Vec2ForCTC,
    TrainingArguments, Trainer, set_seed,
    GenerationConfig               # <- ì‚¬ìš© X, but keep import
)

# ---------- 0. ì¬í˜„ ê°€ëŠ¥ì„± ----------
set_seed(42)

# ---------- 1. ê²½ë¡œ ----------
BASE_DIR  = "/home/coop1964/STT_Korean/dataset"
LABEL_DIR = os.path.join(BASE_DIR, "script")
VOICE_DIR = os.path.join(BASE_DIR, "audio")

# ---------- 2. JSON + WAV ë¡œë“œ ----------
data_list = []
for jf in os.listdir(LABEL_DIR):
    if not jf.endswith(".json"):
        continue
    meta = json.load(open(os.path.join(LABEL_DIR, jf), encoding="utf-8"))

    wav = os.path.join(VOICE_DIR, meta["fileName"])
    txt = meta.get("transcription", "").strip()

    if os.path.exists(wav) and txt:
        data_list.append({"audio": wav, "text": txt})

# 2-â‘ : ìƒ˜í”Œ 30 000ê°œë§Œ ì‚¬ìš©  â˜… ë³€ê²½
random.shuffle(data_list)
data_list = data_list[:30_000]

full_ds = Dataset.from_list(data_list)

# ---------- 3. 8 : 1 : 1 split ----------
split1 = full_ds.train_test_split(test_size=0.2, seed=42)
split2 = split1["test"].train_test_split(test_size=0.5, seed=42)
dataset = DatasetDict(
    train      = split1["train"],
    validation = split2["train"],
    test       = split2["test"]
)

# ---------- 4. ì˜¤ë””ì˜¤ ì»¬ëŸ¼ 16 kHz ----------
dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))

# ---------- 5. Processor & í† í¬ë‚˜ì´ì € ----------
MODEL_NAME = "kresnik/wav2vec2-large-xlsr-korean"
processor  = Wav2Vec2Processor.from_pretrained(MODEL_NAME)

# 5-â‘ : ë¬¸ì¥ë¶€í˜¸ ì¶”ê°€ & tokenizer resize  â˜… ë³€ê²½
added = processor.tokenizer.add_tokens(list(".,?!"))
if added:
    print(f"ğŸ†•  tokenizerì— ë¬¸ì¥ë¶€í˜¸ {added}ê°œ ì¶”ê°€")

# ---------- 6. ì „ì²˜ë¦¬ ----------
def prepare_batch(batch):
    audio = batch["audio"]
    batch["input_values"] = processor(
        audio["array"], sampling_rate=16_000
    ).input_values[0]

    with processor.as_target_processor():
        batch["labels"] = processor(batch["text"]).input_ids
    return batch

dataset = dataset.map(
    prepare_batch,
    remove_columns=["audio", "text"],
    num_proc=4
)

# ---------- 7. DataCollator ----------
@dataclass
class DataCollatorCTCWithPadding:
    processor: Wav2Vec2Processor

    def __call__(self, features):
        inputs  = [torch.tensor(f["input_values"]) for f in features]
        labels  = [torch.tensor(f["labels"])       for f in features]

        batch_inputs = torch.nn.utils.rnn.pad_sequence(
            inputs, batch_first=True
        )
        batch_labels = torch.nn.utils.rnn.pad_sequence(
            labels, batch_first=True, padding_value=-100
        )
        attention_mask = (batch_inputs != 0).long()
        return {"input_values": batch_inputs,
                "attention_mask": attention_mask,
                "labels": batch_labels}


data_collator = DataCollatorCTCWithPadding(processor)

# ---------- 8. ëª¨ë¸ ----------
model = Wav2Vec2ForCTC.from_pretrained(
    MODEL_NAME,
    vocab_size=len(processor.tokenizer),
    pad_token_id=processor.tokenizer.pad_token_id,
    attention_dropout=0.05,
    hidden_dropout=0.05,
    feat_proj_dropout=0.0,
    layerdrop=0.05,
    mask_time_prob=0.04,
    ignore_mismatched_sizes=True
)

if added:  # ìƒˆ í† í°ì´ ì‹¤ì œë¡œ ì¶”ê°€ëì„ ë•Œë§Œ
    old_lm_head = model.lm_head
    in_features = old_lm_head.in_features
    # ìƒˆ vocab í¬ê¸°ë¡œ ìƒˆ Linear ë ˆì´ì–´ ìƒì„±
    model.lm_head = torch.nn.Linear(in_features, len(processor.tokenizer))
    # ê°€ì¤‘ì¹˜ ì´ˆê¸°ê°’ì„ ì´ì „ ê²ƒìœ¼ë¡œ ë³µì‚¬í•˜ê³ , ì¶”ê°€ëœ í† í° ë¶€ë¶„ë§Œ Xavier init
    with torch.no_grad():
        model.lm_head.weight[: old_lm_head.out_features] = old_lm_head.weight
        model.lm_head.bias  [: old_lm_head.out_features] = old_lm_head.bias
        torch.nn.init.xavier_uniform_(model.lm_head.weight[old_lm_head.out_features :])
        torch.nn.init.zeros_(model.lm_head.bias[old_lm_head.out_features :])
    print("âœ…  lm_head ë¦¬ì‚¬ì´ì¦ˆ ì™„ë£Œ")

# ---------- 9. í•˜ì´í¼íŒŒë¼ë¯¸í„° ----------  â˜… ì „ë©´ ìˆ˜ì •
training_args = TrainingArguments(
    output_dir          = "./wav2vec2_finetune_ko_v6",
    evaluation_strategy = "epoch",   # epoch ëë§ˆë‹¤ loss ì¸¡ì •
    save_strategy       = "epoch",
    save_total_limit    = 3,
    num_train_epochs    = 20,        # 15~20 ê¶Œì¥
    per_device_train_batch_size = 4,
    per_device_eval_batch_size  = 4,
    gradient_accumulation_steps = 2, # 4Ã—GPU â†’ ì‹¤íš¨ BS 32
    fp16               = True,
    learning_rate      = 2e-4,       # 1e-4~2e-4 ì•ˆì •
    warmup_ratio       = 0.05,
    lr_scheduler_type  = "cosine",
    dataloader_num_workers = 4,
    logging_steps      = 100,
    report_to          = "none",
    group_by_length=True,
)

trainer = Trainer(
    model           = model,
    args            = training_args,
    data_collator   = data_collator,
    train_dataset   = dataset["train"],
    eval_dataset    = dataset["validation"],
    tokenizer       = processor.feature_extractor,
    # compute_metrics ì‚­ì œ â€” eval_lossë§Œ í™•ì¸
)

# ---------- 10. í•™ìŠµ ----------
trainer.train()

# ---------- 11. ì €ì¥ ----------
CKPT_DIR = "./wav2vec2_korean_v6"
model.save_pretrained(CKPT_DIR)
processor.save_pretrained(CKPT_DIR)
print(f"âœ… ëª¨ë¸Â·í”„ë¡œì„¸ì„œ ì €ì¥: {CKPT_DIR}")

# ---------- 12. ê°„ë‹¨ í‰ê°€ (lossë§Œ) ----------
eval_results = trainer.evaluate(dataset["validation"])
print(f"\nğŸŸ¢  validation loss: {eval_results['eval_loss']:.4f}")
