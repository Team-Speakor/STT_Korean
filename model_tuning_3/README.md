dataset : 8000ê°œ ( 6400/ 800/ 800)

### ðŸ› ï¸ TrainingArguments ì„¤ì •í‘œ

| í•­ëª©                          | ê°’                          | ì„¤ëª…                                                   |
| ----------------------------- | --------------------------- | ------------------------------------------------------ |
| `output_dir`                  | `./wav2vec2_finetune_ko_v5` | ê²°ê³¼ ì €ìž¥ ë””ë ‰í† ë¦¬                                     |
| `num_train_epochs`            | 20                          | ì´ í•™ìŠµ epoch ìˆ˜                                       |
| `per_device_train_batch_size` | 4                           | GPUë‹¹ í•™ìŠµ ë°°ì¹˜ ì‚¬ì´ì¦ˆ                                 |
| `gradient_accumulation_steps` | 8                           | ì‹¤ì œ batch size = 4Ã—8 = 32                             |
| `fp16`                        | True                        | Half precisionìœ¼ë¡œ í•™ìŠµ ì†ë„ í–¥ìƒ ë° ë©”ëª¨ë¦¬ ì ˆì•½ ê°€ëŠ¥  |
| `learning_rate`               | 1e-4                        | ì´ˆê¸° í•™ìŠµë¥                                             |
| `weight_decay`                | 0.005                       | L2 ì •ê·œí™” ê³„ìˆ˜                                         |
| `warmup_ratio`                | 0.05                        | ì „ì²´ step ëŒ€ë¹„ ì›Œë°ì—… ë¹„ìœ¨                             |
| `group_by_length`             | True                        | ì˜¤ë””ì˜¤ ê¸¸ì´ ê¸°ë°˜ dynamic padding                       |
| `eval_strategy`               | `no`                        | OOM ë°©ì§€ë¥¼ ìœ„í•´ í•™ìŠµ ì¤‘ í‰ê°€ ë¹„í™œì„±í™” (ì‚¬í›„ ìˆ˜ë™ í‰ê°€) |
| `eval_steps` / `save_steps`   | 500                         | í‰ê°€ ë° ëª¨ë¸ ì €ìž¥ ì£¼ê¸°                                 |
| `save_total_limit`            | 3                           | ìµœëŒ€ ì €ìž¥ ì²´í¬í¬ì¸íŠ¸ ìˆ˜ ì œí•œ                           |
| `dataloader_num_workers`      | 4                           | ë°ì´í„° ë¡œë”©ì— ì‚¬ìš©í•  ë³‘ë ¬ worker ìˆ˜                    |
| `report_to`                   | `"none"`                    | WandB ë“± ì™¸ë¶€ ë¡œê¹… ë¹„í™œì„±í™”                            |

### code

```python
training_args = TrainingArguments(
output_dir="./wav2vec2_finetune_ko_v5",
eval_steps=500,
save_steps=500,
logging_steps=50,
eval_strategy="steps",
save_total_limit=3,
num_train_epochs=20,
per_device_train_batch_size=4,
 per_device_eval_batch_size=4,
gradient_accumulation_steps=8,
 fp16=True, # AMP ì‚¬ìš©
group_by_length=True,
weight_decay=0.005,
learning_rate=1e-4,
warmup_ratio=0.05,
dataloader_num_workers=4,
report_to="none",
)
```

## PLAN

1. dataset 8000ê°œ -> 30000ê°œ
2. epochs 20 -> 20
3. learning_rate 1e-4 -> 2e-4
